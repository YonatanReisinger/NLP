import json
import pandas as pd
import time
import torch
torch.set_default_device('cpu')

from multiprocessing import Pool, cpu_count
from transformers import AutoModelForCausalLM, AutoTokenizer
from utils.evaluate_results import NO_ANSWER_MARKER, evaluate_results
from utils.prompt_builder import PromptBuilder
from utils.answer_processor import AnswerProcessor
from utils.confidence_scorer import ConfidenceScorer
from utils.span_matcher import SpanMatcher
from utils.analysis_agent import AnalysisAgent


class ModelManager:
    """Responsible for loading the LLM and running inference (single + batch).

    Returns both raw text **and** per-token confidence scores so that
    downstream components can make reject/accept decisions.
    """

    def __init__(self, model_name='meta-llama/Llama-3.2-3B-Instruct'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, dtype=torch.float16, token=True
        )
        self.model.config.pad_token_id = self.tokenizer.eos_token_id
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

    # ── single-item inference ───────────────────────────────────────
    def generate_single(self, messages):
        """Run inference on a single chat-message list.

        Returns:
            str: The raw text generated by the model (new tokens only).
        """
        prompt_text = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, tokenize=False
        )
        inputs = self.tokenizer(
            prompt_text, return_tensors="pt", padding=True, truncation=True
        )

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=50,
                do_sample=False,
            )

        new_tokens = outputs[0][inputs["input_ids"].shape[-1]:]
        return self.tokenizer.decode(new_tokens, skip_special_tokens=True)

    # ── batched inference with confidence scores ────────────────────
    def generate_batch(self, messages_list):
        """Run batched inference and return texts + confidence scores.

        Tokenizes all prompts together with left-padding so that
        generation starts at the same position for every sequence.

        Returns:
            tuple[list[str], list[float]]:
                (raw_texts, confidences) — one entry per input.
        """
        prompt_texts = [
            self.tokenizer.apply_chat_template(
                msgs, add_generation_prompt=True, tokenize=False
            )
            for msgs in messages_list
        ]

        original_padding_side = self.tokenizer.padding_side
        self.tokenizer.padding_side = "left"
        inputs = self.tokenizer(
            prompt_texts, return_tensors="pt", padding=True, truncation=True
        )
        self.tokenizer.padding_side = original_padding_side

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=50,
                do_sample=False,
                return_dict_in_generate=True,
                output_scores=True,
            )

        prompt_len = inputs["input_ids"].shape[1]

        # Decode texts
        texts = []
        for i in range(len(messages_list)):
            new_tokens = outputs.sequences[i][prompt_len:]
            texts.append(
                self.tokenizer.decode(new_tokens, skip_special_tokens=True)
            )

        # Compute confidence scores
        confidence_scorer = ConfidenceScorer()
        confidences = confidence_scorer.compute_batch_confidences(
            outputs.scores, outputs.sequences, prompt_len,
            self.tokenizer.eos_token_id,
        )

        return texts, confidences


class PostProcessor:
    """Worker class for multiprocessing post-processing.

    Similar to NgramProcessor in assignment 1 — holds the components
    needed for post-processing and exposes a process_item() method
    that can be called by Pool.map() across multiple CPU cores.
    """

    def __init__(self, answer_processor, confidence_scorer, span_matcher):
        self.answer_processor = answer_processor
        self.confidence_scorer = confidence_scorer
        self.span_matcher = span_matcher

    def process_item(self, item):
        """Post-process a single (raw_answer, context, confidence) tuple.

        Args:
            item: Tuple of (raw_answer, context, confidence).

        Returns:
            str: The final answer or NO ANSWER.
        """
        raw_answer, context, confidence = item

        # Step 1: regex cleanup + grounding check
        answer = self.answer_processor.process(raw_answer, context)
        if answer == NO_ANSWER_MARKER:
            return answer

        # Step 2: confidence gate
        if not self.confidence_scorer.is_confident(confidence):
            return NO_ANSWER_MARKER

        # Step 3: snap to closest context span
        snapped = self.span_matcher.snap_or_reject(answer, context)
        if snapped is None:
            return NO_ANSWER_MARKER

        return snapped


class SquadQARunner:
    """Orchestrates the full QA pipeline.

    Combines four techniques beyond basic prompting:
        1. Batched parallel inference (ModelManager)
        2. Logit confidence scoring (ConfidenceScorer)
        3. Fuzzy span matching (SpanMatcher)
        4. Rule-based post-processing (AnswerProcessor)

    Uses multiprocessing.Pool (like NgramProcessor in assignment 1)
    to parallelise the post-processing step across CPU cores.
    """

    BATCH_SIZE = 8

    def __init__(self, model_manager, prompt_builder, answer_processor,
                 confidence_scorer, span_matcher, analysis_agent):
        self.model_manager = model_manager
        self.prompt_builder = prompt_builder
        self.answer_processor = answer_processor
        self.confidence_scorer = confidence_scorer
        self.span_matcher = span_matcher
        self.analysis_agent = analysis_agent

    def answer_single(self, context, question):
        """Process one (context, question) pair end-to-end."""
        messages = self.prompt_builder.build_messages(context, question)
        raw_answer = self.model_manager.generate_single(messages)
        return self.answer_processor.process(raw_answer, context)

    def run(self, data_filename):
        """Solve an entire SQuAD 2.0 CSV file.

        Phase 1 — Generation: produce raw answers in batches.
        Phase 2 — Post-processing: use multiprocessing.Pool to
                  post-process all answers in parallel across CPU cores
                  (same pattern as assignment 1's NgramProcessor).

        Returns:
            str: Path to the results CSV.
        """
        df = pd.read_csv(data_filename)
        total = len(df)

        rows = list(df.iterrows())

        # ── Phase 1: Generation (sequential batched) ────────────────
        all_raw_answers = []
        all_contexts = []
        all_confidences = []

        for batch_start in range(0, total, self.BATCH_SIZE):
            batch_rows = rows[batch_start: batch_start + self.BATCH_SIZE]

            messages_list = [
                self.prompt_builder.build_messages(row["context"], row["question"])
                for _, row in batch_rows
            ]
            contexts = [row["context"] for _, row in batch_rows]

            raw_answers, confidences = self.model_manager.generate_batch(
                messages_list
            )

            all_raw_answers.extend(raw_answers)
            all_contexts.extend(contexts)
            all_confidences.extend(confidences)

            # Progress for generation phase
            print(f"  [generated {len(all_raw_answers)}/{total}]")

        # ── Phase 2: Post-processing with multiprocessing ───────────
        num_workers = cpu_count() or 1
        print(f"  Post-processing {total} answers using {num_workers} workers...")

        processor = PostProcessor(
            self.answer_processor,
            self.confidence_scorer,
            self.span_matcher,
        )

        # Build work items — list of (raw_answer, context, confidence) tuples
        work_items = list(zip(all_raw_answers, all_contexts, all_confidences))

        with Pool(processes=num_workers) as pool:
            final_answers = pool.map(processor.process_item, work_items)

        for i, answer in enumerate(final_answers):
            print(f"  [{i + 1}/{total}] {answer[:60]}")

        # ── Phase 3: Analysis Agent verification ─────────────────
        questions = [row["question"] for _, row in rows]
        print(f"  Running Analysis Agent verification on {total} answers...")

        verified_answers = list(final_answers)
        for i in range(total):
            context = all_contexts[i]
            question = questions[i]
            proposed = final_answers[i]
            confidence = all_confidences[i]

            decision = self.analysis_agent.verify(
                context, question, proposed, confidence,
                self.model_manager,
            )

            if proposed == NO_ANSWER_MARKER:
                # Rescue: Agent 2 says REJECT means "NO ANSWER was wrong"
                if decision == "REJECT":
                    # Re-run extraction to try to get an answer
                    msgs = self.prompt_builder.build_messages(context, question)
                    raw = self.model_manager.generate_single(msgs)
                    rescued = self.answer_processor.process(raw, context)
                    if rescued != NO_ANSWER_MARKER:
                        verified_answers[i] = rescued
                        print(f"  [Agent2 rescued {i+1}/{total}] {rescued[:60]}")
            else:
                # Verify: Agent 2 says REJECT means "answer is wrong"
                if decision == "REJECT":
                    verified_answers[i] = NO_ANSWER_MARKER
                    print(f"  [Agent2 rejected {i+1}/{total}] {proposed[:60]}")

            if (i + 1) % 50 == 0:
                print(f"  [verified {i+1}/{total}]")

        print(f"  Analysis Agent verification complete.")

        df["final answer"] = verified_answers

        out_filename = data_filename.replace('.csv', '-results.csv')
        df.to_csv(out_filename, index=False)
        print(f'final answers recorded into {out_filename}')
        return out_filename


# ── Initialise components ────────────────────────────────────────────
model_manager = ModelManager()
prompt_builder = PromptBuilder()
answer_processor = AnswerProcessor()
confidence_scorer = ConfidenceScorer(no_answer_threshold=-1.0)
span_matcher = SpanMatcher(min_similarity=0.6)
analysis_agent = AnalysisAgent()

runner = SquadQARunner(
    model_manager, prompt_builder, answer_processor,
    confidence_scorer, span_matcher, analysis_agent,
)

# Keep legacy references so nothing else breaks
model_name = 'meta-llama/Llama-3.2-3B-Instruct'
tokenizer = model_manager.tokenizer
model = model_manager.model


def generate_answer(messages):
    return model_manager.generate_single(messages)


def answer_single_question(context, question):
    return runner.answer_single(context, question)


def squad_qa(data_filename):
    return runner.run(data_filename)


if __name__ == '__main__':
    start_time = time.time()

    with open('config.json', 'r') as json_file:
        config = json.load(json_file)

    data = pd.read_csv(config['data'])
    sample = data.sample(n=config['sample_for_solution'])  # for grading will be replaced with 'sample_for_grading'
    sample_filename = config['data'].replace('.csv', '-sample.csv')
    sample.to_csv(sample_filename, index=False)

    out_filename = squad_qa(sample_filename)  # todo: the function you implement

    eval_out = evaluate_results(out_filename, final_answer_column='final answer')
    eval_out_list = [str((k, round(v, 3))) for (k, v) in eval_out.items()]
    print('\n'.join(eval_out_list))

    elapsed_time = time.time() - start_time
    print(f"time: {elapsed_time: .2f} sec")
